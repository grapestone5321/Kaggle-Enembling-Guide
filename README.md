# KAGGLE ENSEMBLING GUIDE
ref: https://mlwave.com/kaggle-ensembling-guide/

### Model ensembling is a very powerful technique to increase accuracy on a variety of ML tasks. In this article I will share my ensembling approaches for Kaggle Competitions.

## 1. Creating ensembles from submission files

### 1.1 Voting ensembles.
- Error correcting codes

- A machine learning example

- A pinch of maths

- Number of voters

- Correlation

- Use for Kaggle: Forest Cover Type prediction

- Weighing

- Use for Kaggle: CIFAR-10 Object detection in images

- Code

### 1.2 Averaging
- Kaggle use: Bag of Words Meets Bags of Popcorn

- Code

### 1.3 Rank averaging
- Historical ranks.

- Kaggle use case: Acquire Valued Shoppers Challenge

- Code






## 2. Stacked Generalization & Blending

- Averaging prediction files is nice and easy, but itâ€™s not the only method that the top Kagglers are using. 

- The serious gains start with stacking and blending. 

- Hold on to your top-hats and petticoats: Here be dragons. With 7 heads. 

- Standing on top of 30 other dragons.

### Stacked generalization

### Blending


## 3. Why create these Frankenstein ensembles?
